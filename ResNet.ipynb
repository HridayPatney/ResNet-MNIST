{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eks23Ef-Lhcf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define A Residual Block**\n"
      ],
      "metadata": {
        "id": "x9YzWiDPMC4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,stride=1):\n",
        "    super().__init__()\n",
        "    #first convolution 3x3\n",
        "\n",
        "    self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1, bias=False)\n",
        "    self.bn1=nn.BatchNorm2d(out_channels)\n",
        "    #second convolution  3x3\n",
        "    self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1, bias=False)\n",
        "    self.bn2=nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    #shortcut connection\n",
        "    self.shortcut=nn.Sequential() #gives x itself , it is empty container\n",
        "    if stride !=1 or in_channels!=out_channels:\n",
        "      self.shortcut=nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(out_channels)    #if stride isnt 1 or channels differ we need to project input x to correct size, ie use 1x1 convolutions\n",
        "\n",
        "      )\n",
        "  def forward(self,x):\n",
        "    out=F.relu(self.bn1(self.conv1(x)))\n",
        "    out=self.bn2(self.conv2(out))\n",
        "    out+=self.shortcut(x)\n",
        "    out=F.relu(out)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "EIyKdoolL5zL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# blocks is class - Residual Block\n",
        "#num_blocks is a list with how many blocks to put in each layer\n",
        "#num_classes is number of output classes\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self,block,num_blocks,num_classes=10):\n",
        "    super(ResNet,self).__init__()\n",
        "    self.in_channels = 16\n",
        "\n",
        "    #initial conv\n",
        "    # Adjusted stride and padding for 28x28 input (MNIST)\n",
        "    self.conv1=nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1,bias=False) #in_channels are 1 , out_channels are 16, stride=1 and padding =1 preserves Spatial Size ( 28x28 to 28x28)\n",
        "\n",
        "    self.bn1=nn.BatchNorm2d(16) #we put bias=False because BatchNorm has its own affine shift\n",
        "\n",
        "\n",
        "    #Residual Layers\n",
        "    self.layer1 = self._make_layer(block, 16,  num_blocks[0], stride=1) #outputs 16 channels , use stride =1 (no downsampling)\n",
        "    self.layer2 = self._make_layer(block, 32,  num_blocks[1], stride=2) # outputs 32 channels , first block stride =2 ( downsamples )\n",
        "    self.layer3 = self._make_layer(block, 64,  num_blocks[2], stride=2) # outputs 64 channels , further downsampling\n",
        "\n",
        "    #each nummblocks[i] says how many block instances to put in that layer\n",
        "    #classifier\n",
        "    self.linear=nn.Linear(64,num_classes)\n",
        "\n",
        "  def _make_layer(self,block,out_channels,num_blocks,stride): # helps build on stage ( a sequence of num_blocks residual blocks)\\\n",
        "    strides= [stride]+[1]*(num_blocks-1) # use stride for first block and 1 for rest\n",
        "    layers=[]\n",
        "    for s in strides:\n",
        "      layers.append(block(self.in_channels,out_channels,stride))\n",
        "      self.in_channels=out_channels\n",
        "    return nn.Sequential(*layers)\n",
        "  def forward(self,x):\n",
        "    out=F.relu(self.bn1(self.conv1(x)))\n",
        "    out=self.layer1(out)    # 28x28\n",
        "    out=self.layer2(out)    # 14 x 14\n",
        "    out=self.layer3(out)   # 7x7  , 64 channels\n",
        "    # Adjusted kernel size for average pooling for 28x28 input\n",
        "    out=F.avg_pool2d(out,2)     # 7x7 to 1 feature map has B,64,1,1 # should be 4x4 to 1x1 for 28x28 input\n",
        "    out=out.view(out.size(0),-1)     #flatten to B,64\n",
        "    out=self.linear(out)\n",
        "    return out\n",
        "def ResNetMNIST():\n",
        "  return ResNet(ResidualBlock,[2,2,2],num_classes=10)"
      ],
      "metadata": {
        "id": "dwisuTw5UcPV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Load Data*"
      ],
      "metadata": {
        "id": "EHlRQaHtYWDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compose combines multiple transformations\n",
        "# comvert 28x28 into tensor and normalize by x-0.5/ 0.5\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "#load mnist\n",
        "train_dataset=torchvision.datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "test_dataset=torchvision.datasets.MNIST(root='./data',train=False,download=True,transform=transform)\n",
        "\n",
        "#Data Loader\n",
        "train_loader=DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=1000,shuffle=False)\n"
      ],
      "metadata": {
        "id": "jfxho2UoYBBz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train the model**"
      ],
      "metadata": {
        "id": "4ZU9jAVxZPh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#initialize model,loss,optimizer\n",
        "\n",
        "model=ResNetMNIST().to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  for batch_idx, (images,labels) in enumerate(train_loader):\n",
        "    images,labels=images.to(device),labels.to(device)\n",
        "    #forward pass\n",
        "    outputs=model(images)\n",
        "    loss=criterion(outputs,labels)\n",
        "    #backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcMErBaDZN0o",
        "outputId": "284af3ef-1d4f-4839-e9f1-8bbeb45bc764"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0163\n",
            "Epoch [2/5], Loss: 0.0254\n",
            "Epoch [3/5], Loss: 0.0764\n",
            "Epoch [4/5], Loss: 0.0026\n",
            "Epoch [5/5], Loss: 0.0028\n",
            "Epoch [6/5], Loss: 0.0150\n",
            "Epoch [7/5], Loss: 0.0241\n",
            "Epoch [8/5], Loss: 0.0002\n",
            "Epoch [9/5], Loss: 0.0002\n",
            "Epoch [10/5], Loss: 0.0037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE_ZLystiNnv",
        "outputId": "3978174b-df8b-4ac4-92d3-b7b538b2606c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.05%\n"
          ]
        }
      ]
    }
  ]
}