# ğŸ§  ResNet on MNIST

This project implements a **Residual Neural Network (ResNet)** from scratch in **PyTorch** and trains it on the **MNIST handwritten digit dataset**.  
ResNets, introduced in *He et al. (2015)*, use **skip connections** to make deep neural networks easier to optimize, solving the vanishing gradient problem.

Instead of learning a direct mapping, ResNet blocks learn a **residual function**:

$$
y = F(x) + x
$$

where \(F(x)\) is the learned transformation and \(x\) is the identity mapping (skip connection).

---

## ğŸš€ Features
- âœ… Custom ResNet implementation in PyTorch (no pre-built models used)  
- âœ… MNIST dataset loading with `torchvision` (auto-downloads)  
- âœ… GPU/CPU compatible for Google Colab or local training  
- âœ… Training & evaluation loops with progress + accuracy  
- âœ… Adaptive Average Pooling for handling small 28Ã—28 inputs  

---

## ğŸ“Š Dataset
- **MNIST**: 70,000 handwritten digits (60,000 training + 10,000 testing)  
- Images are grayscale, **28Ã—28 pixels**  
- **10 classes**: digits `0` through `9`

---

## âš™ï¸ Model Architecture
- Initial **Conv + BatchNorm + ReLU**  
- **3 ResNet stages**:
  - Stage 1: 16 channels  
  - Stage 2: 32 channels (downsampling)  
  - Stage 3: 64 channels (downsampling)  
- **Adaptive Average Pooling** â†’ Flatten â†’ Fully Connected Layer â†’ 10 outputs  

---

## ğŸ“ˆ Results
After 5â€“10 epochs of training on GPU (Colab recommended):  
- Training loss decreases steadily  
- Test accuracy reaches **~99%**  

---

## ğŸ”§ How to Run

### 1. Clone Repository
```bash
git clone https://github.com/your-username/resnet-mnist.git
cd resnet-mnist
```
### 2. Install Dependencies
```bash
pip install torch torchvision
```
### 3. Run Training
```bash
python train.py

```
